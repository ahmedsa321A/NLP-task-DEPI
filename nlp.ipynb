{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d043b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m##!pip install nltk\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mnltk\u001b[49m.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m, quiet=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8dbf930",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
    "    \"I hated the film, it was the worst I have ever seen\",\n",
    "    \"The storyline was boring but the acting was brilliant\",\n",
    "    \"An amazing movie with a great plot and incredible performances\",\n",
    "    \"Egypt movie, I regret wasting my time on it\",\n",
    "    \"The actors did a great job but the story lacked depth\",\n",
    "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
    "    \"This film was just okay, not too bad but not great either\",\n",
    "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
    "    \"The movie was disappointing, it did not live up to the hype\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83b33d",
   "metadata": {},
   "source": [
    "## 1. Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d2864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'movie', 'was', 'fantastic', 'and', 'I', 'loved', 'every', 'part', 'of', 'it', 'about', 'Egypt']\n",
      "['I', 'hated', 'the', 'film', ',', 'it', 'was', 'the', 'worst', 'I', 'have', 'ever', 'seen']\n",
      "['The', 'storyline', 'was', 'boring', 'but', 'the', 'acting', 'was', 'brilliant']\n",
      "['An', 'amazing', 'movie', 'with', 'a', 'great', 'plot', 'and', 'incredible', 'performances']\n",
      "['Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'my', 'time', 'on', 'it']\n",
      "['The', 'actors', 'did', 'a', 'great', 'job', 'but', 'the', 'story', 'lacked', 'depth']\n",
      "['One', 'of', 'the', 'best', 'films', 'I', 'have', 'seen', 'in', 'a', 'long', 'time', ',', 'highly', 'recommend', 'it']\n",
      "['This', 'film', 'was', 'just', 'okay', ',', 'not', 'too', 'bad', 'but', 'not', 'great', 'either']\n",
      "['Absolutely', 'loved', 'the', 'movie', ',', 'fantastic', 'plot', 'and', 'wonderful', 'cast']\n",
      "['The', 'movie', 'was', 'disappointing', ',', 'it', 'did', 'not', 'live', 'up', 'to', 'the', 'hype']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_data = [word_tokenize(sentence) for sentence in text_data]\n",
    "for tokens in tokenized_data:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30ad1a",
   "metadata": {},
   "source": [
    "## 2. Stopword Removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f80ef024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e19047d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['The', 'movie', 'was', 'fantastic', 'and', 'I', 'loved', 'every', 'part', 'of', 'it', 'about', 'Egypt']\n",
      "Filtered Tokens: ['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt']\n",
      "--------------------\n",
      "['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt']\n",
      "['hated', 'film', ',', 'worst', 'ever', 'seen']\n",
      "['storyline', 'boring', 'acting', 'brilliant']\n",
      "['amazing', 'movie', 'great', 'plot', 'incredible', 'performances']\n",
      "['Egypt', 'movie', ',', 'regret', 'wasting', 'time']\n",
      "['actors', 'great', 'job', 'story', 'lacked', 'depth']\n",
      "['One', 'best', 'films', 'seen', 'long', 'time', ',', 'highly', 'recommend']\n",
      "['film', 'okay', ',', 'bad', 'great', 'either']\n",
      "['Absolutely', 'loved', 'movie', ',', 'fantastic', 'plot', 'wonderful', 'cast']\n",
      "['movie', 'disappointing', ',', 'live', 'hype']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_data = []\n",
    "for tokens in tokenized_data:\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    filtered_data.append(filtered_tokens)\n",
    "\n",
    "print(\"Original Tokens:\", tokenized_data[0])\n",
    "print(\"Filtered Tokens:\", filtered_data[0])\n",
    "print(\"-\" * 20)\n",
    "for tokens in filtered_data:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e24a3",
   "metadata": {},
   "source": [
    "## 3. Stemming and Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "761707e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens:  ['amazing', 'movie', 'great', 'plot', 'incredible', 'performances']\n",
      "Stemmed Tokens:   ['amaz', 'movi', 'great', 'plot', 'incred', 'perform']\n",
      "--------------------\n",
      "['movi', 'fantast', 'love', 'everi', 'part', 'egypt']\n",
      "['hate', 'film', ',', 'worst', 'ever', 'seen']\n",
      "['storylin', 'bore', 'act', 'brilliant']\n",
      "['amaz', 'movi', 'great', 'plot', 'incred', 'perform']\n",
      "['egypt', 'movi', ',', 'regret', 'wast', 'time']\n",
      "['actor', 'great', 'job', 'stori', 'lack', 'depth']\n",
      "['one', 'best', 'film', 'seen', 'long', 'time', ',', 'highli', 'recommend']\n",
      "['film', 'okay', ',', 'bad', 'great', 'either']\n",
      "['absolut', 'love', 'movi', ',', 'fantast', 'plot', 'wonder', 'cast']\n",
      "['movi', 'disappoint', ',', 'live', 'hype']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_data = [[stemmer.stem(word) for word in tokens] for tokens in filtered_data]\n",
    "\n",
    "print(\"Filtered Tokens: \", filtered_data[3])\n",
    "print(\"Stemmed Tokens:  \", stemmed_data[3])\n",
    "print(\"-\" * 20)\n",
    "\n",
    "for tokens in stemmed_data:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0603818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfb2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens:    ['amazing', 'movie', 'great', 'plot', 'incredible', 'performances']\n",
      "Lemmatized Tokens:  ['amazing', 'movie', 'great', 'plot', 'incredible', 'performance']\n",
      "--------------------\n",
      "['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt']\n",
      "['hated', 'film', ',', 'worst', 'ever', 'seen']\n",
      "['storyline', 'boring', 'acting', 'brilliant']\n",
      "['amazing', 'movie', 'great', 'plot', 'incredible', 'performance']\n",
      "['Egypt', 'movie', ',', 'regret', 'wasting', 'time']\n",
      "['actor', 'great', 'job', 'story', 'lacked', 'depth']\n",
      "['One', 'best', 'film', 'seen', 'long', 'time', ',', 'highly', 'recommend']\n",
      "['film', 'okay', ',', 'bad', 'great', 'either']\n",
      "['Absolutely', 'loved', 'movie', ',', 'fantastic', 'plot', 'wonderful', 'cast']\n",
      "['movie', 'disappointing', ',', 'live', 'hype']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_data = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in filtered_data]\n",
    "\n",
    "print(\"Filtered Tokens:   \", filtered_data[3])\n",
    "print(\"Lemmatized Tokens: \", lemmatized_data[3])\n",
    "print(\"-\" * 20)\n",
    "\n",
    "for tokens in lemmatized_data:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca58f2",
   "metadata": {},
   "source": [
    "## 4. Part-of-Speech (POS) Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0348639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2947efbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "633f0a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['The', 'movie', 'was', 'fantastic', 'and', 'I', 'loved', 'every', 'part', 'of', 'it', 'about', 'Egypt']\n",
      "POS Tagged:      [('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('fantastic', 'JJ'), ('and', 'CC'), ('I', 'PRP'), ('loved', 'VBD'), ('every', 'DT'), ('part', 'NN'), ('of', 'IN'), ('it', 'PRP'), ('about', 'IN'), ('Egypt', 'NNP')]\n",
      "--------------------\n",
      "Original Tokens: ['I', 'hated', 'the', 'film', ',', 'it', 'was', 'the', 'worst', 'I', 'have', 'ever', 'seen']\n",
      "POS Tagged:      [('I', 'PRP'), ('hated', 'VBD'), ('the', 'DT'), ('film', 'NN'), (',', ','), ('it', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('worst', 'JJS'), ('I', 'PRP'), ('have', 'VBP'), ('ever', 'RB'), ('seen', 'VBN')]\n",
      "--------------------\n",
      "Original Tokens: ['The', 'storyline', 'was', 'boring', 'but', 'the', 'acting', 'was', 'brilliant']\n",
      "POS Tagged:      [('The', 'DT'), ('storyline', 'NN'), ('was', 'VBD'), ('boring', 'VBG'), ('but', 'CC'), ('the', 'DT'), ('acting', 'NN'), ('was', 'VBD'), ('brilliant', 'JJ')]\n",
      "--------------------\n",
      "Original Tokens: ['An', 'amazing', 'movie', 'with', 'a', 'great', 'plot', 'and', 'incredible', 'performances']\n",
      "POS Tagged:      [('An', 'DT'), ('amazing', 'JJ'), ('movie', 'NN'), ('with', 'IN'), ('a', 'DT'), ('great', 'JJ'), ('plot', 'NN'), ('and', 'CC'), ('incredible', 'JJ'), ('performances', 'NNS')]\n",
      "--------------------\n",
      "Original Tokens: ['Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'my', 'time', 'on', 'it']\n",
      "POS Tagged:      [('Egypt', 'NNP'), ('movie', 'NN'), (',', ','), ('I', 'PRP'), ('regret', 'VBP'), ('wasting', 'VBG'), ('my', 'PRP$'), ('time', 'NN'), ('on', 'IN'), ('it', 'PRP')]\n",
      "--------------------\n",
      "Original Tokens: ['The', 'actors', 'did', 'a', 'great', 'job', 'but', 'the', 'story', 'lacked', 'depth']\n",
      "POS Tagged:      [('The', 'DT'), ('actors', 'NNS'), ('did', 'VBD'), ('a', 'DT'), ('great', 'JJ'), ('job', 'NN'), ('but', 'CC'), ('the', 'DT'), ('story', 'NN'), ('lacked', 'VBD'), ('depth', 'NN')]\n",
      "--------------------\n",
      "Original Tokens: ['One', 'of', 'the', 'best', 'films', 'I', 'have', 'seen', 'in', 'a', 'long', 'time', ',', 'highly', 'recommend', 'it']\n",
      "POS Tagged:      [('One', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('films', 'NNS'), ('I', 'PRP'), ('have', 'VBP'), ('seen', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('long', 'JJ'), ('time', 'NN'), (',', ','), ('highly', 'RB'), ('recommend', 'VB'), ('it', 'PRP')]\n",
      "--------------------\n",
      "Original Tokens: ['This', 'film', 'was', 'just', 'okay', ',', 'not', 'too', 'bad', 'but', 'not', 'great', 'either']\n",
      "POS Tagged:      [('This', 'DT'), ('film', 'NN'), ('was', 'VBD'), ('just', 'RB'), ('okay', 'RB'), (',', ','), ('not', 'RB'), ('too', 'RB'), ('bad', 'JJ'), ('but', 'CC'), ('not', 'RB'), ('great', 'JJ'), ('either', 'CC')]\n",
      "--------------------\n",
      "Original Tokens: ['Absolutely', 'loved', 'the', 'movie', ',', 'fantastic', 'plot', 'and', 'wonderful', 'cast']\n",
      "POS Tagged:      [('Absolutely', 'RB'), ('loved', 'VBD'), ('the', 'DT'), ('movie', 'NN'), (',', ','), ('fantastic', 'JJ'), ('plot', 'NN'), ('and', 'CC'), ('wonderful', 'JJ'), ('cast', 'NN')]\n",
      "--------------------\n",
      "Original Tokens: ['The', 'movie', 'was', 'disappointing', ',', 'it', 'did', 'not', 'live', 'up', 'to', 'the', 'hype']\n",
      "POS Tagged:      [('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('disappointing', 'JJ'), (',', ','), ('it', 'PRP'), ('did', 'VBD'), ('not', 'RB'), ('live', 'VB'), ('up', 'RB'), ('to', 'TO'), ('the', 'DT'), ('hype', 'NN')]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "# Apply POS tagging to each tokenized review\n",
    "pos_tagged_data = [pos_tag(tokens) for tokens in tokenized_data]\n",
    "\n",
    "# Print the first sentence and its tagged version for a clear example\n",
    "for original, tagged in zip(tokenized_data, pos_tagged_data):\n",
    "    print(\"Original Tokens:\", original)\n",
    "    print(\"POS Tagged:     \", tagged)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e4c76",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition (NER):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0ca593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e59361ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  movie/NN\n",
      "  was/VBD\n",
      "  fantastic/JJ\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  loved/VBD\n",
      "  every/DT\n",
      "  part/NN\n",
      "  of/IN\n",
      "  it/PRP\n",
      "  about/IN\n",
      "  (GPE Egypt/NNP))\n",
      "(S\n",
      "  I/PRP\n",
      "  hated/VBD\n",
      "  the/DT\n",
      "  film/NN\n",
      "  ,/,\n",
      "  it/PRP\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  worst/JJS\n",
      "  I/PRP\n",
      "  have/VBP\n",
      "  ever/RB\n",
      "  seen/VBN)\n",
      "(S\n",
      "  The/DT\n",
      "  storyline/NN\n",
      "  was/VBD\n",
      "  boring/VBG\n",
      "  but/CC\n",
      "  the/DT\n",
      "  acting/NN\n",
      "  was/VBD\n",
      "  brilliant/JJ)\n",
      "(S\n",
      "  An/DT\n",
      "  amazing/JJ\n",
      "  movie/NN\n",
      "  with/IN\n",
      "  a/DT\n",
      "  great/JJ\n",
      "  plot/NN\n",
      "  and/CC\n",
      "  incredible/JJ\n",
      "  performances/NNS)\n",
      "(S\n",
      "  (GPE Egypt/NNP)\n",
      "  movie/NN\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  regret/VBP\n",
      "  wasting/VBG\n",
      "  my/PRP$\n",
      "  time/NN\n",
      "  on/IN\n",
      "  it/PRP)\n",
      "(S\n",
      "  The/DT\n",
      "  actors/NNS\n",
      "  did/VBD\n",
      "  a/DT\n",
      "  great/JJ\n",
      "  job/NN\n",
      "  but/CC\n",
      "  the/DT\n",
      "  story/NN\n",
      "  lacked/VBD\n",
      "  depth/NN)\n",
      "(S\n",
      "  One/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  best/JJS\n",
      "  films/NNS\n",
      "  I/PRP\n",
      "  have/VBP\n",
      "  seen/VBN\n",
      "  in/IN\n",
      "  a/DT\n",
      "  long/JJ\n",
      "  time/NN\n",
      "  ,/,\n",
      "  highly/RB\n",
      "  recommend/VB\n",
      "  it/PRP)\n",
      "(S\n",
      "  This/DT\n",
      "  film/NN\n",
      "  was/VBD\n",
      "  just/RB\n",
      "  okay/RB\n",
      "  ,/,\n",
      "  not/RB\n",
      "  too/RB\n",
      "  bad/JJ\n",
      "  but/CC\n",
      "  not/RB\n",
      "  great/JJ\n",
      "  either/CC)\n",
      "(S\n",
      "  Absolutely/RB\n",
      "  loved/VBD\n",
      "  the/DT\n",
      "  movie/NN\n",
      "  ,/,\n",
      "  fantastic/JJ\n",
      "  plot/NN\n",
      "  and/CC\n",
      "  wonderful/JJ\n",
      "  cast/NN)\n",
      "(S\n",
      "  The/DT\n",
      "  movie/NN\n",
      "  was/VBD\n",
      "  disappointing/JJ\n",
      "  ,/,\n",
      "  it/PRP\n",
      "  did/VBD\n",
      "  not/RB\n",
      "  live/VB\n",
      "  up/RB\n",
      "  to/TO\n",
      "  the/DT\n",
      "  hype/NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "\n",
    "\n",
    "ner_trees = [ne_chunk(tagged_tokens) for tagged_tokens in pos_tagged_data]\n",
    "\n",
    "# Print the resulting NER trees\n",
    "for tree in ner_trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc04b63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame preview:\n",
      "                                     original_review  \\\n",
      "0  The movie was fantastic and I loved every part...   \n",
      "1  I hated the film, it was the worst I have ever...   \n",
      "2  The storyline was boring but the acting was br...   \n",
      "3  An amazing movie with a great plot and incredi...   \n",
      "4        Egypt movie, I regret wasting my time on it   \n",
      "\n",
      "                        stemmed_review  \\\n",
      "0   movi fantast love everi part egypt   \n",
      "1          hate film , worst ever seen   \n",
      "2          storylin bore act brilliant   \n",
      "3  amaz movi great plot incred perform   \n",
      "4        egypt movi , regret wast time   \n",
      "\n",
      "                                 lemmatized_review  \n",
      "0           movie fantastic loved every part Egypt  \n",
      "1                     hated film , worst ever seen  \n",
      "2                storyline boring acting brilliant  \n",
      "3  amazing movie great plot incredible performance  \n",
      "4                Egypt movie , regret wasting time  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stemmed_reviews = [\" \".join(tokens) for tokens in stemmed_data]\n",
    "lemmatized_reviews = [\" \".join(tokens) for tokens in lemmatized_data]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'original_review': text_data,\n",
    "    'stemmed_review': stemmed_reviews,\n",
    "    'lemmatized_review': lemmatized_reviews\n",
    "})\n",
    "\n",
    "print(\"DataFrame preview:\")\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv('reviews_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bd7d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    review_index original_token stemmed_token lemmatized_token pos_tag\n",
      "0              0          movie          movi            movie      DT\n",
      "1              0      fantastic       fantast        fantastic      NN\n",
      "2              0          loved          love            loved     VBD\n",
      "3              0          every         everi            every      JJ\n",
      "4              0           part          part             part      CC\n",
      "5              0          Egypt         egypt            Egypt     PRP\n",
      "6              1          hated          hate            hated     PRP\n",
      "7              1           film          film             film     VBD\n",
      "8              1              ,             ,                ,      DT\n",
      "9              1          worst         worst            worst      NN\n",
      "10             1           ever          ever             ever       ,\n",
      "11             1           seen          seen             seen     PRP\n",
      "12             2      storyline      storylin        storyline      DT\n",
      "13             2         boring          bore           boring      NN\n",
      "14             2         acting           act           acting     VBD\n",
      "15             2      brilliant     brilliant        brilliant     VBG\n",
      "16             3        amazing          amaz          amazing      DT\n",
      "17             3          movie          movi            movie      JJ\n",
      "18             3          great         great            great      NN\n",
      "19             3           plot          plot             plot      IN\n",
      "20             3     incredible        incred       incredible      DT\n",
      "21             3   performances       perform      performance      JJ\n",
      "22             4          Egypt         egypt            Egypt     NNP\n",
      "23             4          movie          movi            movie      NN\n",
      "24             4              ,             ,                ,       ,\n",
      "25             4         regret        regret           regret     PRP\n",
      "26             4        wasting          wast          wasting     VBP\n",
      "27             4           time          time             time     VBG\n",
      "28             5         actors         actor            actor      DT\n",
      "29             5          great         great            great     NNS\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "token_rows = []\n",
    "\n",
    "for i, (orig_tokens, stem_tokens, lemma_tokens, pos_tags) in enumerate(zip(filtered_data, stemmed_data, lemmatized_data, pos_tagged_data)):\n",
    "    for orig, stem, lemma, (token, tag) in zip(orig_tokens, stem_tokens, lemma_tokens, pos_tags):\n",
    "        token_rows.append({\n",
    "            'review_index': i,\n",
    "            'original_token': orig,\n",
    "            'stemmed_token': stem,\n",
    "            'lemmatized_token': lemma,\n",
    "            'pos_tag': tag\n",
    "        })\n",
    "\n",
    "tokens_df = pd.DataFrame(token_rows)\n",
    "tokens_df.to_csv('tokens_processed.csv', index=False)\n",
    "print(tokens_df.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a289b",
   "metadata": {},
   "source": [
    "## LAB 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8ffad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FY2021 Q1', 'fy2020 Q4']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = '''\n",
    "The gross cost of operating lease vehicles in FY2021 Q1 was\n",
    "$4.85 billion.\n",
    "In previous quarter i.e. fy2020 Q4 it was $3 billion fy2020 Q5.\n",
    "'''\n",
    "\n",
    "result = re.findall(r\"fy\\d{4}\\sQ[1-4]\", text, re.IGNORECASE)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dbcb19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elonmusk', 'teslarati', 'dummy_tesla', 'dummy_2_tesla']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Follow our leader Elon musk on twitter here:\n",
    "https://twitter.com/elonmusk, more information\n",
    "on Tesla's products can be found at https://www.tesla.com/. Also\n",
    "here are leading influencers\n",
    "for tesla related news,\n",
    "https://twitter.com/teslarati\n",
    "https://twitter.com/dummy_tesla\n",
    "https://twitter.com/dummy_2_tesla\n",
    "'''\n",
    "\n",
    "handles = re.findall(r\"https://twitter.com/(\\w+)\", text)\n",
    "\n",
    "handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018ac8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info@daturial.com',\n",
       " 'support@daturial.org.',\n",
       " 'sarah.ali2025@gmail.com',\n",
       " 'contact@ai-solutions.co.uk',\n",
       " 'admission@university.academy',\n",
       " 'baraa.eng@example.com']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text =\"\"\"\n",
    "Hello everyone, \n",
    "My name is Baraa from the Daturial team. Last\n",
    "week we received many messages from students interested in Artificial\n",
    "Intelligence. Some contacted us via the official email info@daturial.com, while\n",
    "others used the support address support@daturial.org. \n",
    "One student wrote: “My name is Ahmed, my phone\n",
    "number is 01012345678, and I want to join the upcoming course.” Meanwhile, his\n",
    "colleague Sarah sent her email sarah.ali2025@gmail.com and said she prefers\n",
    "phone communication: 01555555555. \n",
    "We also received a message from a consulting\n",
    "company using the address contact@ai-solutions.co.uk, which included their\n",
    "website link www.ai-solutions.co.uk. In addition, a university contacted us\n",
    "from admission@university.academy asking about student discounts. \n",
    "We noticed that some messages only had links\n",
    "such as www.example.com or random numbers like 778899 and a date 2025/09/20.\n",
    "These are not valid emails or phone numbers. \n",
    "On the other hand, we published an official\n",
    "announcement on our website daturial.com stating that the course begins on\n",
    "01-10-2025 and ends on 30-11-2025. The course fee is 5000 EGP, and payment can\n",
    "be made via bank transfer to account number 123456789012 or with a Visa\n",
    "card. \n",
    "Anyone needing more details can email us at\n",
    "baraa.eng@example.com or visit the FAQ page at www.daturial.com/faq. \n",
    "Finally, we thank everyone who contacted us\n",
    "via email or phone. Please note that any message not signed from domains like\n",
    "daturial.com or daturial.org will not be answered.\n",
    "\"\"\"\n",
    "\n",
    "emails = re.findall(r\"[\\w.-]+@[a-zA-Z0-9-]+\\.[a-zA-Z.-]+\", text)\n",
    "\n",
    "emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3430ed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01012345678', '01555555555']\n"
     ]
    }
   ],
   "source": [
    "phone_numbers = re.findall(r\"\\b01\\d{9}\\b\", text)\n",
    "\n",
    "print(phone_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello everyone, \n",
      "My name is Baraa from the Daturial team. Last\n",
      "week we received many messages from students interested in Artificial\n",
      "Intelligence. Some contacted us via the official email [EMAIL HIDDEN], while\n",
      "others used the support address [EMAIL HIDDEN] \n",
      "One student wrote: “My name is Ahmed, my phone\n",
      "number is 01012345678, and I want to join the upcoming course.” Meanwhile, his\n",
      "colleague Sarah sent her email [EMAIL HIDDEN] and said she prefers\n",
      "phone communication: 01555555555. \n",
      "We also received a message from a consulting\n",
      "company using the address [EMAIL HIDDEN], which included their\n",
      "website link www.ai-solutions.co.uk. In addition, a university contacted us\n",
      "from [EMAIL HIDDEN] asking about student discounts. \n",
      "We noticed that some messages only had links\n",
      "such as www.example.com or random numbers like 778899 and a date 2025/09/20.\n",
      "These are not valid emails or phone numbers. \n",
      "On the other hand, we published an official\n",
      "announcement on our website daturial.com stating that the course begins on\n",
      "01-10-2025 and ends on 30-11-2025. The course fee is 5000 EGP, and payment can\n",
      "be made via bank transfer to account number 123456789012 or with a Visa\n",
      "card. \n",
      "Anyone needing more details can email us at\n",
      "[EMAIL HIDDEN] or visit the FAQ page at www.daturial.com/faq. \n",
      "Finally, we thank everyone who contacted us\n",
      "via email or phone. Please note that any message not signed from domains like\n",
      "daturial.com or daturial.org will not be answered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_emails_text = re.sub(r\"[\\w.-]+@[a-zA-Z0-9-]+\\.[a-zA-Z.-]+\", \"[EMAIL HIDDEN]\", text)\n",
    "\n",
    "hidden_emails_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f934a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daturial.com',\n",
       " 'daturial.org',\n",
       " 'gmail.com',\n",
       " 'www.ai-solutions.co.uk',\n",
       " 'www.example.com',\n",
       " 'daturial.com',\n",
       " 'example.com',\n",
       " 'www.daturial.com/faq',\n",
       " 'daturial.com',\n",
       " 'daturial.org']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = re.findall(r\"\\b(www\\.[\\w\\.\\-/]+|[\\w-]+\\.(?:com|org))\\b\", text)\n",
    "urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
