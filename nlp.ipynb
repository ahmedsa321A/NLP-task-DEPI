{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d043b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m##!pip install nltk\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mnltk\u001b[49m.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m, quiet=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8dbf930",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\n",
    "    \"The movie was fantastic and I loved every part of it about Egypt\",\n",
    "    \"I hated the film, it was the worst I have ever seen\",\n",
    "    \"The storyline was boring but the acting was brilliant\",\n",
    "    \"An amazing movie with a great plot and incredible performances\",\n",
    "    \"Egypt movie, I regret wasting my time on it\",\n",
    "    \"The actors did a great job but the story lacked depth\",\n",
    "    \"One of the best films I have seen in a long time, highly recommend it\",\n",
    "    \"This film was just okay, not too bad but not great either\",\n",
    "    \"Absolutely loved the movie, fantastic plot and wonderful cast\",\n",
    "    \"The movie was disappointing, it did not live up to the hype\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83b33d",
   "metadata": {},
   "source": [
    "## 1. Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d2864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'movie', 'was', 'fantastic', 'and', 'I', 'loved', 'every', 'part', 'of', 'it', 'about', 'Egypt']\n",
      "['I', 'hated', 'the', 'film', ',', 'it', 'was', 'the', 'worst', 'I', 'have', 'ever', 'seen']\n",
      "['The', 'storyline', 'was', 'boring', 'but', 'the', 'acting', 'was', 'brilliant']\n",
      "['An', 'amazing', 'movie', 'with', 'a', 'great', 'plot', 'and', 'incredible', 'performances']\n",
      "['Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'my', 'time', 'on', 'it']\n",
      "['The', 'actors', 'did', 'a', 'great', 'job', 'but', 'the', 'story', 'lacked', 'depth']\n",
      "['One', 'of', 'the', 'best', 'films', 'I', 'have', 'seen', 'in', 'a', 'long', 'time', ',', 'highly', 'recommend', 'it']\n",
      "['This', 'film', 'was', 'just', 'okay', ',', 'not', 'too', 'bad', 'but', 'not', 'great', 'either']\n",
      "['Absolutely', 'loved', 'the', 'movie', ',', 'fantastic', 'plot', 'and', 'wonderful', 'cast']\n",
      "['The', 'movie', 'was', 'disappointing', ',', 'it', 'did', 'not', 'live', 'up', 'to', 'the', 'hype']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_data = [word_tokenize(sentence) for sentence in text_data]\n",
    "for tokens in tokenized_data:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30ad1a",
   "metadata": {},
   "source": [
    "## 2. Stopword Removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f80ef024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e19047d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['The', 'movie', 'was', 'fantastic', 'and', 'I', 'loved', 'every', 'part', 'of', 'it', 'about', 'Egypt']\n",
      "Filtered Tokens: ['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt']\n",
      "--------------------\n",
      "['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt']\n",
      "['hated', 'film', ',', 'worst', 'ever', 'seen']\n",
      "['storyline', 'boring', 'acting', 'brilliant']\n",
      "['amazing', 'movie', 'great', 'plot', 'incredible', 'performances']\n",
      "['Egypt', 'movie', ',', 'regret', 'wasting', 'time']\n",
      "['actors', 'great', 'job', 'story', 'lacked', 'depth']\n",
      "['One', 'best', 'films', 'seen', 'long', 'time', ',', 'highly', 'recommend']\n",
      "['film', 'okay', ',', 'bad', 'great', 'either']\n",
      "['Absolutely', 'loved', 'movie', ',', 'fantastic', 'plot', 'wonderful', 'cast']\n",
      "['movie', 'disappointing', ',', 'live', 'hype']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_data = []\n",
    "for tokens in tokenized_data:\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    filtered_data.append(filtered_tokens)\n",
    "\n",
    "print(\"Original Tokens:\", tokenized_data[0])\n",
    "print(\"Filtered Tokens:\", filtered_data[0])\n",
    "print(\"-\" * 20)\n",
    "for tokens in filtered_data:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e24a3",
   "metadata": {},
   "source": [
    "## 3. Stemming and Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "761707e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens:  ['amazing', 'movie', 'great', 'plot', 'incredible', 'performances']\n",
      "Stemmed Tokens:   ['amaz', 'movi', 'great', 'plot', 'incred', 'perform']\n",
      "--------------------\n",
      "['movi', 'fantast', 'love', 'everi', 'part', 'egypt']\n",
      "['hate', 'film', ',', 'worst', 'ever', 'seen']\n",
      "['storylin', 'bore', 'act', 'brilliant']\n",
      "['amaz', 'movi', 'great', 'plot', 'incred', 'perform']\n",
      "['egypt', 'movi', ',', 'regret', 'wast', 'time']\n",
      "['actor', 'great', 'job', 'stori', 'lack', 'depth']\n",
      "['one', 'best', 'film', 'seen', 'long', 'time', ',', 'highli', 'recommend']\n",
      "['film', 'okay', ',', 'bad', 'great', 'either']\n",
      "['absolut', 'love', 'movi', ',', 'fantast', 'plot', 'wonder', 'cast']\n",
      "['movi', 'disappoint', ',', 'live', 'hype']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_data = [[stemmer.stem(word) for word in tokens] for tokens in filtered_data]\n",
    "\n",
    "print(\"Filtered Tokens: \", filtered_data[3])\n",
    "print(\"Stemmed Tokens:  \", stemmed_data[3])\n",
    "print(\"-\" * 20)\n",
    "\n",
    "for tokens in stemmed_data:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0603818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfb2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens:    ['amazing', 'movie', 'great', 'plot', 'incredible', 'performances']\n",
      "Lemmatized Tokens:  ['amazing', 'movie', 'great', 'plot', 'incredible', 'performance']\n",
      "--------------------\n",
      "['movie', 'fantastic', 'loved', 'every', 'part', 'Egypt']\n",
      "['hated', 'film', ',', 'worst', 'ever', 'seen']\n",
      "['storyline', 'boring', 'acting', 'brilliant']\n",
      "['amazing', 'movie', 'great', 'plot', 'incredible', 'performance']\n",
      "['Egypt', 'movie', ',', 'regret', 'wasting', 'time']\n",
      "['actor', 'great', 'job', 'story', 'lacked', 'depth']\n",
      "['One', 'best', 'film', 'seen', 'long', 'time', ',', 'highly', 'recommend']\n",
      "['film', 'okay', ',', 'bad', 'great', 'either']\n",
      "['Absolutely', 'loved', 'movie', ',', 'fantastic', 'plot', 'wonderful', 'cast']\n",
      "['movie', 'disappointing', ',', 'live', 'hype']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_data = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in filtered_data]\n",
    "\n",
    "print(\"Filtered Tokens:   \", filtered_data[3])\n",
    "print(\"Lemmatized Tokens: \", lemmatized_data[3])\n",
    "print(\"-\" * 20)\n",
    "\n",
    "for tokens in lemmatized_data:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca58f2",
   "metadata": {},
   "source": [
    "## 4. Part-of-Speech (POS) Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0348639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2947efbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "633f0a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['The', 'movie', 'was', 'fantastic', 'and', 'I', 'loved', 'every', 'part', 'of', 'it', 'about', 'Egypt']\n",
      "POS Tagged:      [('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('fantastic', 'JJ'), ('and', 'CC'), ('I', 'PRP'), ('loved', 'VBD'), ('every', 'DT'), ('part', 'NN'), ('of', 'IN'), ('it', 'PRP'), ('about', 'IN'), ('Egypt', 'NNP')]\n",
      "--------------------\n",
      "Original Tokens: ['I', 'hated', 'the', 'film', ',', 'it', 'was', 'the', 'worst', 'I', 'have', 'ever', 'seen']\n",
      "POS Tagged:      [('I', 'PRP'), ('hated', 'VBD'), ('the', 'DT'), ('film', 'NN'), (',', ','), ('it', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('worst', 'JJS'), ('I', 'PRP'), ('have', 'VBP'), ('ever', 'RB'), ('seen', 'VBN')]\n",
      "--------------------\n",
      "Original Tokens: ['The', 'storyline', 'was', 'boring', 'but', 'the', 'acting', 'was', 'brilliant']\n",
      "POS Tagged:      [('The', 'DT'), ('storyline', 'NN'), ('was', 'VBD'), ('boring', 'VBG'), ('but', 'CC'), ('the', 'DT'), ('acting', 'NN'), ('was', 'VBD'), ('brilliant', 'JJ')]\n",
      "--------------------\n",
      "Original Tokens: ['An', 'amazing', 'movie', 'with', 'a', 'great', 'plot', 'and', 'incredible', 'performances']\n",
      "POS Tagged:      [('An', 'DT'), ('amazing', 'JJ'), ('movie', 'NN'), ('with', 'IN'), ('a', 'DT'), ('great', 'JJ'), ('plot', 'NN'), ('and', 'CC'), ('incredible', 'JJ'), ('performances', 'NNS')]\n",
      "--------------------\n",
      "Original Tokens: ['Egypt', 'movie', ',', 'I', 'regret', 'wasting', 'my', 'time', 'on', 'it']\n",
      "POS Tagged:      [('Egypt', 'NNP'), ('movie', 'NN'), (',', ','), ('I', 'PRP'), ('regret', 'VBP'), ('wasting', 'VBG'), ('my', 'PRP$'), ('time', 'NN'), ('on', 'IN'), ('it', 'PRP')]\n",
      "--------------------\n",
      "Original Tokens: ['The', 'actors', 'did', 'a', 'great', 'job', 'but', 'the', 'story', 'lacked', 'depth']\n",
      "POS Tagged:      [('The', 'DT'), ('actors', 'NNS'), ('did', 'VBD'), ('a', 'DT'), ('great', 'JJ'), ('job', 'NN'), ('but', 'CC'), ('the', 'DT'), ('story', 'NN'), ('lacked', 'VBD'), ('depth', 'NN')]\n",
      "--------------------\n",
      "Original Tokens: ['One', 'of', 'the', 'best', 'films', 'I', 'have', 'seen', 'in', 'a', 'long', 'time', ',', 'highly', 'recommend', 'it']\n",
      "POS Tagged:      [('One', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('films', 'NNS'), ('I', 'PRP'), ('have', 'VBP'), ('seen', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('long', 'JJ'), ('time', 'NN'), (',', ','), ('highly', 'RB'), ('recommend', 'VB'), ('it', 'PRP')]\n",
      "--------------------\n",
      "Original Tokens: ['This', 'film', 'was', 'just', 'okay', ',', 'not', 'too', 'bad', 'but', 'not', 'great', 'either']\n",
      "POS Tagged:      [('This', 'DT'), ('film', 'NN'), ('was', 'VBD'), ('just', 'RB'), ('okay', 'RB'), (',', ','), ('not', 'RB'), ('too', 'RB'), ('bad', 'JJ'), ('but', 'CC'), ('not', 'RB'), ('great', 'JJ'), ('either', 'CC')]\n",
      "--------------------\n",
      "Original Tokens: ['Absolutely', 'loved', 'the', 'movie', ',', 'fantastic', 'plot', 'and', 'wonderful', 'cast']\n",
      "POS Tagged:      [('Absolutely', 'RB'), ('loved', 'VBD'), ('the', 'DT'), ('movie', 'NN'), (',', ','), ('fantastic', 'JJ'), ('plot', 'NN'), ('and', 'CC'), ('wonderful', 'JJ'), ('cast', 'NN')]\n",
      "--------------------\n",
      "Original Tokens: ['The', 'movie', 'was', 'disappointing', ',', 'it', 'did', 'not', 'live', 'up', 'to', 'the', 'hype']\n",
      "POS Tagged:      [('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('disappointing', 'JJ'), (',', ','), ('it', 'PRP'), ('did', 'VBD'), ('not', 'RB'), ('live', 'VB'), ('up', 'RB'), ('to', 'TO'), ('the', 'DT'), ('hype', 'NN')]\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "# Apply POS tagging to each tokenized review\n",
    "pos_tagged_data = [pos_tag(tokens) for tokens in tokenized_data]\n",
    "\n",
    "# Print the first sentence and its tagged version for a clear example\n",
    "for original, tagged in zip(tokenized_data, pos_tagged_data):\n",
    "    print(\"Original Tokens:\", original)\n",
    "    print(\"POS Tagged:     \", tagged)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e4c76",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition (NER):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0ca593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e59361ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  movie/NN\n",
      "  was/VBD\n",
      "  fantastic/JJ\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  loved/VBD\n",
      "  every/DT\n",
      "  part/NN\n",
      "  of/IN\n",
      "  it/PRP\n",
      "  about/IN\n",
      "  (GPE Egypt/NNP))\n",
      "(S\n",
      "  I/PRP\n",
      "  hated/VBD\n",
      "  the/DT\n",
      "  film/NN\n",
      "  ,/,\n",
      "  it/PRP\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  worst/JJS\n",
      "  I/PRP\n",
      "  have/VBP\n",
      "  ever/RB\n",
      "  seen/VBN)\n",
      "(S\n",
      "  The/DT\n",
      "  storyline/NN\n",
      "  was/VBD\n",
      "  boring/VBG\n",
      "  but/CC\n",
      "  the/DT\n",
      "  acting/NN\n",
      "  was/VBD\n",
      "  brilliant/JJ)\n",
      "(S\n",
      "  An/DT\n",
      "  amazing/JJ\n",
      "  movie/NN\n",
      "  with/IN\n",
      "  a/DT\n",
      "  great/JJ\n",
      "  plot/NN\n",
      "  and/CC\n",
      "  incredible/JJ\n",
      "  performances/NNS)\n",
      "(S\n",
      "  (GPE Egypt/NNP)\n",
      "  movie/NN\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  regret/VBP\n",
      "  wasting/VBG\n",
      "  my/PRP$\n",
      "  time/NN\n",
      "  on/IN\n",
      "  it/PRP)\n",
      "(S\n",
      "  The/DT\n",
      "  actors/NNS\n",
      "  did/VBD\n",
      "  a/DT\n",
      "  great/JJ\n",
      "  job/NN\n",
      "  but/CC\n",
      "  the/DT\n",
      "  story/NN\n",
      "  lacked/VBD\n",
      "  depth/NN)\n",
      "(S\n",
      "  One/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  best/JJS\n",
      "  films/NNS\n",
      "  I/PRP\n",
      "  have/VBP\n",
      "  seen/VBN\n",
      "  in/IN\n",
      "  a/DT\n",
      "  long/JJ\n",
      "  time/NN\n",
      "  ,/,\n",
      "  highly/RB\n",
      "  recommend/VB\n",
      "  it/PRP)\n",
      "(S\n",
      "  This/DT\n",
      "  film/NN\n",
      "  was/VBD\n",
      "  just/RB\n",
      "  okay/RB\n",
      "  ,/,\n",
      "  not/RB\n",
      "  too/RB\n",
      "  bad/JJ\n",
      "  but/CC\n",
      "  not/RB\n",
      "  great/JJ\n",
      "  either/CC)\n",
      "(S\n",
      "  Absolutely/RB\n",
      "  loved/VBD\n",
      "  the/DT\n",
      "  movie/NN\n",
      "  ,/,\n",
      "  fantastic/JJ\n",
      "  plot/NN\n",
      "  and/CC\n",
      "  wonderful/JJ\n",
      "  cast/NN)\n",
      "(S\n",
      "  The/DT\n",
      "  movie/NN\n",
      "  was/VBD\n",
      "  disappointing/JJ\n",
      "  ,/,\n",
      "  it/PRP\n",
      "  did/VBD\n",
      "  not/RB\n",
      "  live/VB\n",
      "  up/RB\n",
      "  to/TO\n",
      "  the/DT\n",
      "  hype/NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "\n",
    "\n",
    "ner_trees = [ne_chunk(tagged_tokens) for tagged_tokens in pos_tagged_data]\n",
    "\n",
    "# Print the resulting NER trees\n",
    "for tree in ner_trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc04b63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame preview:\n",
      "                                     original_review  \\\n",
      "0  The movie was fantastic and I loved every part...   \n",
      "1  I hated the film, it was the worst I have ever...   \n",
      "2  The storyline was boring but the acting was br...   \n",
      "3  An amazing movie with a great plot and incredi...   \n",
      "4        Egypt movie, I regret wasting my time on it   \n",
      "\n",
      "                        stemmed_review  \\\n",
      "0   movi fantast love everi part egypt   \n",
      "1          hate film , worst ever seen   \n",
      "2          storylin bore act brilliant   \n",
      "3  amaz movi great plot incred perform   \n",
      "4        egypt movi , regret wast time   \n",
      "\n",
      "                                 lemmatized_review  \n",
      "0           movie fantastic loved every part Egypt  \n",
      "1                     hated film , worst ever seen  \n",
      "2                storyline boring acting brilliant  \n",
      "3  amazing movie great plot incredible performance  \n",
      "4                Egypt movie , regret wasting time  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stemmed_reviews = [\" \".join(tokens) for tokens in stemmed_data]\n",
    "lemmatized_reviews = [\" \".join(tokens) for tokens in lemmatized_data]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'original_review': text_data,\n",
    "    'stemmed_review': stemmed_reviews,\n",
    "    'lemmatized_review': lemmatized_reviews\n",
    "})\n",
    "\n",
    "print(\"DataFrame preview:\")\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv('reviews_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bd7d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    review_index original_token stemmed_token lemmatized_token pos_tag\n",
      "0              0            The          movi            movie      DT\n",
      "1              0          movie       fantast        fantastic      NN\n",
      "2              0            was          love            loved     VBD\n",
      "3              0      fantastic         everi            every      JJ\n",
      "4              0            and          part             part      CC\n",
      "5              0              I         egypt            Egypt     PRP\n",
      "6              1              I          hate            hated     PRP\n",
      "7              1          hated          film             film     VBD\n",
      "8              1            the             ,                ,      DT\n",
      "9              1           film         worst            worst      NN\n",
      "10             1              ,          ever             ever       ,\n",
      "11             1             it          seen             seen     PRP\n",
      "12             2            The      storylin        storyline      DT\n",
      "13             2      storyline          bore           boring      NN\n",
      "14             2            was           act           acting     VBD\n",
      "15             2         boring     brilliant        brilliant     VBG\n",
      "16             3             An          amaz          amazing      DT\n",
      "17             3        amazing          movi            movie      JJ\n",
      "18             3          movie         great            great      NN\n",
      "19             3           with          plot             plot      IN\n",
      "20             3              a        incred       incredible      DT\n",
      "21             3          great       perform      performance      JJ\n",
      "22             4          Egypt         egypt            Egypt     NNP\n",
      "23             4          movie          movi            movie      NN\n",
      "24             4              ,             ,                ,       ,\n",
      "25             4              I        regret           regret     PRP\n",
      "26             4         regret          wast          wasting     VBP\n",
      "27             4        wasting          time             time     VBG\n",
      "28             5            The         actor            actor      DT\n",
      "29             5         actors         great            great     NNS\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "token_rows = []\n",
    "\n",
    "for i, (orig_tokens, stem_tokens, lemma_tokens, pos_tags) in enumerate(zip(tokenized_data, stemmed_data, lemmatized_data, pos_tagged_data)):\n",
    "    for orig, stem, lemma, (token, tag) in zip(orig_tokens, stem_tokens, lemma_tokens, pos_tags):\n",
    "        token_rows.append({\n",
    "            'review_index': i,\n",
    "            'original_token': orig,\n",
    "            'stemmed_token': stem,\n",
    "            'lemmatized_token': lemma,\n",
    "            'pos_tag': tag\n",
    "        })\n",
    "\n",
    "tokens_df = pd.DataFrame(token_rows)\n",
    "tokens_df.to_csv('tokens_processed.csv', index=False)\n",
    "print(tokens_df.head(30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
